#!/usr/bin/env python3
"""
Unified dataset preprocessing for AtomGPT, CDVAE, and FlowMM.

Usage
-----
Choose a sub-command:

    python data_preprocess.py atomgpt  --dataset dft_3d --output ./atomgpt_out  \
                                       --target Tc_supercon --max-size 1000 --seed 123

    python data_preprocess.py cdvae    --dataset dft_3d --output ./cdvae_out    \
                                       --target Tc_supercon --max-size 1000 --seed 123

    python data_preprocess.py flowmm   --dataset dft_3d --output ./flowmm_out   \
                                       --target Tc_supercon --max-size 1000 --seed 123

The same *shuffle-based* split (controlled by --seed) is used for every
sub-command, so each model sees identical train/val/test partitions. POSCAR
files are always written. CSV formats match the original reference scripts.

Special-case hygiene for JARVIS Supercon-3D (dataset dft_3d):
  - Remove the duplicate entry 'JVASP-19919' BEFORE shuffling/splitting so it
    cannot land in train/val/test (it is structurally identical to another entry).
  - Remove known leakage-prone *test* IDs (structure duplicates of train entries)
    AFTER shuffling/splitting, so the split is minimally perturbed while the
    leaked test exemplars are excluded across *all* model outputs:
      * JVASP-20425
      * JVASP-16080

Abstract-factory layout:

    ┌────────────────────────┐
    │   DataPrepFactory (ABC)│
    ├────────────────────────┤
    │ +write_outputs(df, …)  │  <- implemented differently per model
    └──────────▲─────────────┘
               │
     ┌─────────┼─────────┬─────────┐
     │         │         │         │
 AtomGPTFactory│ CDVAEFactory│ FlowMMFactory
               │         │         │
               ▼         ▼         ▼
      id_prop.csv   train/val/test train/val/test
                    (minimal cols) (extended cols)
"""
from __future__ import annotations

import os
import argparse
import json
import random
import textwrap
from abc import ABC, abstractmethod
from hashlib import sha256
from pathlib import Path
from typing import List, Tuple, Optional, Dict

import pandas as pd
from tqdm import tqdm

from jarvis.db.figshare import data as jarvis_data
from jarvis.core.atoms import Atoms
from jarvis.io.vasp.inputs import Poscar
from pymatgen.core import Structure
from pymatgen.symmetry.analyzer import SpacegroupAnalyzer
from contextlib import contextmanager


@contextmanager
def _pushd(path: Path):
    prev = os.getcwd()
    os.chdir(path)
    try:
        yield
    finally:
        os.chdir(prev)


################################################################################
# Utility helpers
################################################################################

def deterministic_split(
    n_samples: int,
    val_ratio: float,
    test_ratio: float,
    seed: int,
) -> Tuple[List[int], List[int], List[int]]:
    """Return (id_train, id_val, id_test) as lists of indices."""
    indices = list(range(n_samples))
    random.seed(seed)
    random.shuffle(indices)

    n_val = int(val_ratio * n_samples)
    n_test = int(test_ratio * n_samples)
    n_train = n_samples - n_val - n_test

    id_train = indices[:n_train]
    id_val = indices[n_train : n_train + n_val]
    id_test = indices[n_train + n_val :]
    return id_train, id_val, id_test


def hash10(values: List[str]) -> str:
    h = sha256()
    for v in values:
        h.update(v.encode())
        h.update(b",")
    return h.hexdigest()[:10]


################################################################################
# Known leakage hygiene (Supercon-3D)
################################################################################

# These IDs were observed in the test split to be STRUCTURE-hash duplicates of train entries.
# Keep the train exemplars; exclude these *test* exemplars.
SUPERCON3D_DROP_FROM_TEST_IDS = {"JVASP-20425", "JVASP-16080"}


def maybe_drop_known_test_leakage_ids(
    df: pd.DataFrame,
    dataset_name: str,
    id_test: List[int],
) -> List[int]:
    """
    Dataset-specific hygiene: for Supercon-3D only, remove specific known-leakage IDs
    from the *test* split while preserving order of the remaining test indices.
    """
    if dataset_name.strip().lower() != "dft_3d":
        return id_test
    if "material_id" not in df.columns:
        return id_test

    kept: List[int] = []
    dropped: List[str] = []
    for idx in id_test:
        jid = str(df.iloc[idx]["material_id"]).strip()
        if jid in SUPERCON3D_DROP_FROM_TEST_IDS:
            dropped.append(jid)
        else:
            kept.append(idx)

    if dropped:
        uniq = sorted(set(dropped))
        print(
            f"⚠️  Supercon-3D leakage hygiene: removed {len(dropped)} test row(s) "
            f"({len(uniq)} unique IDs) from test split: {uniq}"
        )
    return kept


def purge_stale_leakage_artifacts(out_dir: Path, dataset_name: str) -> None:
    """
    If outputs already exist from a previous run, ensure the leakage IDs' POSCAR files
    are not lingering in the output directory.
    """
    if dataset_name.strip().lower() != "dft_3d":
        return
    for jid in SUPERCON3D_DROP_FROM_TEST_IDS:
        f = out_dir / jid  # POSCAR filenames are bare JIDs in this pipeline
        if f.exists():
            try:
                f.unlink()
                print(f"⚠️  Removed stale crystal file: {f}")
            except Exception as e:
                print(f"⚠️  Could not remove stale crystal file {f}: {e}")


def canonicalise(pmg_struct: Structure, symprec: float = 0.1):
    """Return (cif_raw, cif_conv, spg_num, spg_conv). Never raises."""
    try:
        sga = SpacegroupAnalyzer(pmg_struct, symprec=symprec)
        spg_num = sga.get_space_group_number()
        cif_conv = sga.get_conventional_standard_structure().to(fmt="cif")
        spg_conv = SpacegroupAnalyzer(
            Structure.from_str(cif_conv, fmt="cif"), symprec=symprec
        ).get_space_group_number()
        return pmg_struct.to(fmt="cif"), cif_conv, spg_num, spg_conv
    except Exception:
        return "", "", -1, -1


def maybe_drop_known_duplicates(df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:
    """
    Dataset-specific hygiene.

    For JARVIS Supercon-3D (dataset_name == 'dft_3d'), drop JVASP-19919
    (known duplicate / structurally identical to another entry), *before* shuffling.
    """
    if dataset_name.strip().lower() != "dft_3d":
        return df

    bad_id = "JVASP-19919"
    if "material_id" not in df.columns:
        return df

    mask = df["material_id"].astype(str) != bad_id
    dropped = int((~mask).sum())
    if dropped > 0:
        print(f"⚠️  Supercon-3D hygiene: dropped {dropped} row(s) with material_id='{bad_id}' before splitting.")
    else:
        print(f"✓ Supercon-3D hygiene: '{bad_id}' not present; nothing to drop.")
    return df.loc[mask].reset_index(drop=True)


################################################################################
# Abstract-factory base class
################################################################################

class DataPrepFactory(ABC):
    """Abstract product: writes model-specific outputs."""

    def __init__(self, out_dir: Path, target_key: str):
        self.out_dir = out_dir
        self.target_key = target_key
        self.out_dir.mkdir(parents=True, exist_ok=True)

    # Shared step – always write POSCARs
    def write_poscar_files(self, df: pd.DataFrame, indices: List[int]) -> List[str]:
        rel_paths: List[str] = []

        # change directory temporarily → CWD becomes self.out_dir
        with _pushd(self.out_dir):
            for idx in indices:
                jid = df.iloc[idx]["material_id"]
                fname = f"{jid}"

                # Jarvis now writes both copies *inside* self.out_dir,
                # which are actually the same file, so only ONE file appears.
                Poscar(df.iloc[idx]["atoms_j"]).write_file(fname)
                rel_paths.append(fname)

        return rel_paths

    @abstractmethod
    def write_outputs(
        self,
        df: pd.DataFrame,
        id_train: List[int],
        id_val: List[int],
        id_test: List[int],
    ) -> None:  # pragma: no cover
        ...


################################################################################
# Concrete factories
################################################################################

class AtomGPTFactory(DataPrepFactory):
    """Produces id_prop.csv compatible with AtomGPT."""

    def write_outputs(self, df, id_train, id_val, id_test):
        # POSCARs – write once for all ids
        id_all = id_train + id_val + id_test
        rel_paths = self.write_poscar_files(df, id_all)

        # Build id_prop.csv – keep the same order across models
        targets = [df.iloc[i][self.target_key] for i in id_all]
        id_prop = pd.DataFrame({"structure_path": rel_paths, self.target_key: targets})
        id_prop.to_csv(self.out_dir / "id_prop.csv", index=False, header=False)

        print(f"✓ AtomGPT: wrote {len(id_prop)} rows -> {self.out_dir / 'id_prop.csv'}")
        print(f"   hash10(ids)={hash10(rel_paths)}")


class CDVAEFactory(DataPrepFactory):
    """Produces train/val/test CSVs (minimal columns) for CDVAE."""

    def write_outputs(self, df, id_train, id_val, id_test):
        # POSCARs – optional for CDVAE but user requested; reuse helper
        self.write_poscar_files(df, id_train + id_val + id_test)

        def make_df(id_lst: List[int]):
            return pd.DataFrame(
                {
                    "material_id": [df.iloc[i]["material_id"] for i in id_lst],
                    "cif": [df.iloc[i]["cif_raw"] for i in id_lst],
                    self.target_key: [df.iloc[i][self.target_key] for i in id_lst],
                }
            )

        make_df(id_train).to_csv(self.out_dir / "train.csv", index=False)
        make_df(id_val).to_csv(self.out_dir / "val.csv", index=False)
        make_df(id_test).to_csv(self.out_dir / "test.csv", index=False)

        print(
            "✓ CDVAE: wrote train/val/test CSVs in", self.out_dir,
            " hashes", hash10([df.iloc[i]["material_id"] for i in id_test])
        )


class FlowMMFactory(DataPrepFactory):
    """Produces train/val/test CSVs with extra descriptors for FlowMM."""

    def write_outputs(self, df, id_train, id_val, id_test):
        # POSCARs – again, for completeness
        self.write_poscar_files(df, id_train + id_val + id_test)

        def make_df(id_lst: List[int]):
            return pd.DataFrame(
                {
                    "material_id": [df.iloc[i]["material_id"] for i in id_lst],
                    "pretty_formula": [df.iloc[i]["pretty_formula"] for i in id_lst],
                    "elements": [json.dumps(df.iloc[i]["elements"]) for i in id_lst],
                    "cif": [df.iloc[i]["cif_raw"] for i in id_lst],
                    "spacegroup.number": [df.iloc[i]["spg_raw"] for i in id_lst],
                    "spacegroup.number.conv": [df.iloc[i]["spg_conv"] for i in id_lst],
                    "cif.conv": [df.iloc[i]["cif_conv"] for i in id_lst],
                    self.target_key: [df.iloc[i][self.target_key] for i in id_lst],
                }
            )

        make_df(id_train).to_csv(self.out_dir / "train.csv", index=False)
        make_df(id_val).to_csv(self.out_dir / "val.csv", index=False)
        make_df(id_test).to_csv(self.out_dir / "test.csv", index=False)

        print(
            "✓ FlowMM: wrote train/val/test CSVs in", self.out_dir,
            " hashes", hash10([df.iloc[i]["material_id"] for i in id_test])
        )


################################################################################
# Dataset acquisition (shared for all factories)
################################################################################

def collect_records(
    dataset_name: str,
    id_key: str,
    target_key: str,
    max_size: Optional[int],
) -> pd.DataFrame:
    """Download dataset, keep those with a valid *target_key*.

    Returned DataFrame columns:
        material_id, atoms_j (jarvis Atoms), cif_raw, cif_conv, spg_raw,
        spg_conv, pretty_formula, elements (list[str]), target_key
    """
    records: List[Dict] = []
    for item in tqdm(jarvis_data(dataset_name), desc="Downloading/JARVIS"):
        tgt = item.get(target_key, "na")
        if tgt in ("na", None):
            continue

        # Build structural objects
        atoms_j = Atoms.from_dict(item["atoms"])
        pmg: Structure = atoms_j.pymatgen_converter()

        try:
            cif_raw, cif_conv, spg_raw, spg_conv = canonicalise(pmg)
            if not cif_raw:  # canonicalise failed badly
                continue
        except Exception:
            continue

        records.append(
            {
                "material_id": item[id_key],
                "atoms_j": atoms_j,
                "cif_raw": cif_raw,
                "cif_conv": cif_conv,
                "spg_raw": spg_raw,
                "spg_conv": spg_conv,
                "pretty_formula": pmg.composition.reduced_formula,
                "elements": [el.symbol for el in pmg.species],
                target_key: tgt,
            }
        )

        if max_size is not None and len(records) == max_size:
            break

    df = pd.DataFrame(records)
    if df.empty:
        raise RuntimeError(
            f"No usable entries found for dataset '{dataset_name}' with target "
            f"'{target_key}'."
        )
    return df


################################################################################
# CLI parsing and wiring
################################################################################

FACTORY_REGISTRY = {
    "atomgpt": AtomGPTFactory,
    "cdvae": CDVAEFactory,
    "flowmm": FlowMMFactory,
}


def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(
        prog="data_preprocess",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description=textwrap.dedent(
            """Create consistent train/val/test splits for AtomGPT, CDVAE, and FlowMM.
            The split is controlled by --seed; use the same seed to guarantee
            identical partitions across models.
            """
        ),
    )

    common = argparse.ArgumentParser(add_help=False)
    common.add_argument("--dataset", required=True, help="JARVIS dataset name")
    common.add_argument("--target", dest="target_key", default="Tc_supercon")
    common.add_argument("--id-key", default="jid")
    common.add_argument("--output", required=True, help="Output directory")
    common.add_argument("--max-size", type=int, default=None)
    common.add_argument("--seed", type=int, default=123)
    common.add_argument("--val-ratio", type=float, default=0.1)
    common.add_argument("--test-ratio", type=float, default=0.1)

    sub = p.add_subparsers(dest="model", required=True)

    for name in FACTORY_REGISTRY:
        sub.add_parser(name, parents=[common], help=f"prepare data for {name}")

    return p


def main(argv: Optional[List[str]] = None) -> None:  # entry-point
    args = build_parser().parse_args(argv)

    # 1) Collect data (shared)
    df = collect_records(
        dataset_name=args.dataset,
        id_key=args.id_key,
        target_key=args.target_key,
        max_size=args.max_size,
    )

    # 1b) Dataset-specific hygiene (drop known duplicate before splitting)
    df = maybe_drop_known_duplicates(df, dataset_name=args.dataset)

    n_samples = len(df)
    print(f"✓ Collected {n_samples} usable entries")

    # 2) Compute deterministic split
    id_train, id_val, id_test = deterministic_split(
        n_samples,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio,
        seed=args.seed,
    )
    print(f"Split sizes (pre-hygiene)  train:{len(id_train)}  val:{len(id_val)}  test:{len(id_test)}")

    # 2b) Supercon-3D leakage hygiene: remove specified IDs from test only (keep order)
    id_test = maybe_drop_known_test_leakage_ids(df, dataset_name=args.dataset, id_test=id_test)
    print(f"Split sizes (post-hygiene) train:{len(id_train)}  val:{len(id_val)}  test:{len(id_test)}")

    # 3) Dispatch to the requested factory
    out_dir = Path(args.output)
    out_dir.mkdir(parents=True, exist_ok=True)
    purge_stale_leakage_artifacts(out_dir, dataset_name=args.dataset)

    factory_cls = FACTORY_REGISTRY[args.model]
    factory = factory_cls(out_dir, args.target_key)
    factory.write_outputs(df, id_train, id_val, id_test)


if __name__ == "__main__":
    main()

